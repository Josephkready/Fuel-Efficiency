{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Beginner Exercise - Fuel Efficiency .ipynb","version":"0.3.2","provenance":[{"file_id":"https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/keras/basic_regression.ipynb","timestamp":1558700120018}],"private_outputs":true,"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"EIdT9iu_Z4Rb"},"source":["# Fuel Efficiency\n","\n","Source: https://www.tensorflow.org/alpha/tutorials/keras/basic_regression\n","\n","Dataset: https://archive.ics.uci.edu/ml/datasets/auto+mpg"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"AHp3M9ZmrIxj"},"source":["We are going to user the [Auto MPG](https://archive.ics.uci.edu/ml/datasets/auto+mpg) Dataset to predict the fuel efficiency of late-1970s and early 1980s automobiles. To do this we'll use Tensorflow 2.0 and Keras. \n","\n","First, install our libraries: \n","* seaborn\n","* matplotlib pyplot\n","* pandas\n","* Tensorflow"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"moB4tpEHxKB3","colab":{}},"source":["!pip install tensorflow==2.0.0-alpha0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"1rRo8oNqZ-Rj","colab":{}},"source":["from __future__ import absolute_import, division, print_function, unicode_literals\n","\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import seaborn as sns\n","\n","import tensorflow as tf\n","\n","print(tf.__version__)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"F_72b0LCNbjx"},"source":["## The Auto MPG dataset\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"gFh9ne3FZ-On"},"source":["### Get the data\n","We will need to download the dataset using[ tf.keras.utils.get_file ](https://www.tensorflow.org/api_docs/python/tf/keras/utils/get_file)\n","\n","Link to the dataset: http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"p9kxxgzvzlyz","colab":{}},"source":["dataset_path = tf.keras.utils.get_file(\"auto-mpg.data\", \"http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"nslsRLh7Zss4"},"source":["Now that the file is saved on our Google Colab, we will need to import it into a Pandas Dataframe so we can manipulate it. Use [ pd.read_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) to import it. Then display the first 5 so we can see what the dataset looks like"]},{"cell_type":"code","metadata":{"id":"hYa78t3x0rpY","colab_type":"code","colab":{}},"source":["raw_dataset = pd.read_csv(dataset_path)\n","raw_dataset.head()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tnsIPYn-06AH","colab_type":"text"},"source":["Something is off with this dataset. Let's see if we can clean things up using the parameters in [ pd.read_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html). \n","\n","Define: \n","* column_names\n","* na_values\n","* comment\n","* sep\n","* skipinitialspace\n","\n","Check out the documentation on [ pd.read_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) to see what all these parameters do. Once you've imported the data, display the first five again. "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"CiX2FI4gZtTt","colab":{}},"source":["column_names = ['MPG','Cylinders','Displacement','Horsepower','Weight',\n","                'Acceleration', 'Model Year', 'Origin']\n","dataset = pd.read_csv(dataset_path, names=column_names,\n","                      na_values = \"?\", comment='\\t',\n","                      sep=\" \", skipinitialspace=True)\n","\n","dataset.head()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"3MWuJTKEDM-f"},"source":["### Clean the data\n","\n","First, we'll need to check for missing data in our dataset. Pandas has a built-in function for this [isna](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.isna.html). Run this function on the dataset and take the sum to see if there is any missing data."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"JEJHhN65a2VV","colab":{}},"source":["dataset.isna().sum()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w_3QRjx5xP1u","colab_type":"text"},"source":["Looks like there are some missing values. Let's get rid of them by using [dropna](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html) function. Since we are dropping so few, it won't have a large impact on our dataset. "]},{"cell_type":"code","metadata":{"id":"QuMXn6FMxQuo","colab_type":"code","colab":{}},"source":["dataset = dataset.dropna()\n","dataset.isna().sum()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"8XKitwaH4v8h"},"source":["Next, the `\"Origin\"` column is really categorical, not numeric. We will need to convert this using **one hot encoding**. Pandas has a built in function [get_dummies](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html) that can help us here."]},{"cell_type":"code","metadata":{"id":"ma2_ErHO3xhC","colab_type":"code","colab":{}},"source":["dataset = pd.get_dummies(dataset, columns=['Origin'], dtype=float)\n","dataset.head()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BLEAqSiw-bYW","colab_type":"text"},"source":["As you'll notice, `get_dummies` did not have the correct column names. Let's update these manually using Pandas [rename](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rename.html) function. We will need to create a dictionary with the correct names. \n","\n","Note that:\n","* USA = 1\n","* Europe = 2\n","* Japan = 3"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"gWNTD2QjBWFJ","colab":{}},"source":["origin_names = {'Origin_1':'USA', 'Origin_2':'Europe', 'Origin_3': 'Japan'}\n","dataset.rename(index=str, columns=origin_names, inplace=True)\n","dataset.head()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"J4ubs136WLNp"},"source":["##Exploring the Data\n"]},{"cell_type":"markdown","metadata":{"id":"_xC6ZefjLNLe","colab_type":"text"},"source":["### Graphs\n","Before we create our label datasets, let's explore the Auto MPG dataset. We can use seaborn's [pairplot](https://seaborn.pydata.org/generated/seaborn.pairplot.html) to automatically build graphs displaying our features. \n","\n","Plot out:\n","- MPG\n","- Cylinders\n","- Displacement \n","- Weight"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"oRKO_x8gWKv-","colab":{}},"source":["sns.pairplot(dataset[[\"MPG\", \"Cylinders\", \"Displacement\", \"Weight\"]], diag_kind=\"kde\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jNsIrvKFHGLK","colab_type":"text"},"source":["Here are some interesting things to note from the graphs:\n","* Most cars are 4, 6, or 8 cylinder cars, with a few 3 and 5 cylinder cars. \n","* The cylinders vs MPG graph shows us a linear trend, more cylinders = less gas mileage. \n","* We also see the same trend with displacement vs MPG and weight vs MPG. \n","* More cylinders = more displacement = more weight. \n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"gavKO_6DWRMP"},"source":["### Statistics\n","We can also look at the overall statistics of our dataset using Pandas [describe](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html) and [transpose](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.transpose.html) functions:\n","\n","*Don't forget to pop off the MPG*"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"yi2FzC3T21jR","colab":{}},"source":["stats = dataset.describe()\n","stats.pop(\"MPG\")\n","stats = stats.transpose()\n","stats"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZNg_O_StO4YQ","colab_type":"text"},"source":["##Training and Testing"]},{"cell_type":"markdown","metadata":{"id":"XUcBsKT3JB1Z","colab_type":"text"},"source":["### Split the data into train and test\n","\n","Now split the dataset into a training set and a test set. We can use the Pandas [sample](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sample.html) function to grab a random 80% selection from the dataset. \n","\n","We can then use Pandas [drop](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html) function to grab the remaining 20% to use as our testing data. \n","\n","Don't forget to [pop](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.pop.html) off the labels as well. "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"t2sluJdCW7jN","colab":{}},"source":["train_data = dataset.sample(frac=0.8,)\n","test_data = dataset.drop(train_data.index)\n","train_labels = train_data.pop('MPG')\n","test_labels = test_data.pop('MPG')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"mRklxK5s388r"},"source":["### Normalize the data\n","\n","Look again at the `stats` block above and note how different the ranges of each feature are."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"-ywmerQ6dSox"},"source":["We want to create a function to normalize the data. Since we want to determine the relation of our features to MPG, we need to normalize our feature values so they are within the same range and our model isn't thrown off by data in a different range. \n","\n","To accomplish this, we will build a function that subtracts from our data the mean / standard deviation. \n","\n","For example: data - (mean / standard deviation)"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"JlC5ooJrgjQF","colab":{}},"source":["def norm(x):\n","  return (x - stats['mean']) / stats['std']\n","normed_train_data = norm(train_data)\n","normed_test_data = norm(test_data)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"BuiClDk45eS4"},"source":["This normalized data is what we will use to train the model.\n","\n","Caution: The statistics used to normalize the inputs here (mean and standard deviation) need to be applied to any other data that is fed to the model, along with the one-hot encoding that we did earlier.  That includes the test data."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"SmjdzxKzEu1-"},"source":["## The model"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"6SWtkIjhrZwa"},"source":["### Build the model\n","\n","Let's build our model. Here, we'll use a `Sequential` model with two densely connected hidden layers, and an output layer that returns a single, continuous value. We will need to define the layers and compile the model.\n","\n","For the compile we are going to define a custom optimizer, the [tf.keras.optimizers.RMSprop](https://keras.io/optimizers/) with a learning rate of .001. This is an unpublished adaptive learning rate optimization algorithm designed for Neural Networks. Click [here](https://towardsdatascience.com/understanding-rmsprop-faster-neural-network-learning-62e116fcf29a) for more information.\n","\n","Since this is a regression problem, we'll use [ mean squared error](https://en.wikipedia.org/wiki/Mean_squared_error) and[ mean absolute error](https://en.wikipedia.org/wiki/Mean_absolute_error) as our metrics. *Hint: Keras abbreviates these to mse and mae*\n"]},{"cell_type":"code","metadata":{"id":"nThBww0-Qy3B","colab_type":"code","colab":{}},"source":["model = tf.keras.Sequential([\n","    tf.keras.layers.Dense(64, activation='relu', input_shape=[len(train_data.keys())]),\n","    tf.keras.layers.Dense(64, activation='relu'),\n","    tf.keras.layers.Dense(1)\n","  ])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"y_PsvyicQ5Vp","colab_type":"code","colab":{}},"source":["#Custom Optimizer\n","optimizer = tf.keras.optimizers.RMSprop(0.001)\n","\n","model.compile(loss='mse',\n","              optimizer=optimizer,\n","              metrics=['mae', 'mse'])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Sj49Og4YGULr"},"source":["### Inspect the model\n","\n","Use the `.summary` method to print a simple description of the model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ReAD0n6MsFK-","colab":{}},"source":["model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Vt6W50qGsJAL"},"source":["\n","Now try out the model. Take a batch of `10` examples from the training data and call `model.predict` on it. These predictions might seem far off. Remember these predictions are before we trained the model. "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"-d-gBaVtGTSC","colab":{}},"source":["example_batch = train_data[:10]\n","example_result = model.predict(example_batch)\n","example_result"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0-qWCsh6DlyH"},"source":["### Train the model\n","\n","Train the model for 1000 epochs and record the training and validation accuracy in the `history` object. Take 20% of our training data to use for validation. **Remember** to pass in our normalized data!\n","\n","While fitting our model we are going to define a custom callback. 1000 epochs is a lot and we don't want the training ticks taking up all of our screen. We'll create a custom class using [tf.keras.callbacks.Callback](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback) that will allow us to change what displays after the completed epoch. \n","\n","We are also going to define [EarlyStopping](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping). This callback will stop the model training after x number of epochs to see if it's improving. \n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"sD7qHCmNIOY0","colab":{}},"source":["# Display training progress by printing a single dot for each completed epoch\n","class PrintDot(tf.keras.callbacks.Callback):\n","  def on_epoch_end(self, epoch, logs):\n","    if epoch % 100 == 0: print('')\n","    print('.', end='')\n","    \n","# The patience parameter is the amount of epochs to check for improvement\n","early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n","\n","\n","history = model.fit(\n","  normed_train_data, train_labels,\n","  epochs=1000, validation_split = 0.2, verbose=0,\n","  callbacks=[early_stop, PrintDot()])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"tQm3pc0FYPQB"},"source":["Note that with the Early Stop defined, our model trains for much fewer epochs\n","\n","Now that the model is finished, we can visualize the model's training progress using the stats stored in the `history` object. \n","\n","1.   Import the history into a pandas dataframe\n","2.   Create a column for the epochs\n","3.   Look at the end of the dataframe\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"4Xj91b-dymEy","colab":{}},"source":["hist = pd.DataFrame(history.history)\n","hist['epoch'] = history.epoch\n","hist.tail()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"93kmn-Pxdih4","colab_type":"text"},"source":["Using the following code, was can take the mean squared error and mean absolute error from the history object and graph it. "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"B6XriGbVPh2t","colab":{}},"source":["def plot_history(history):\n","  hist = pd.DataFrame(history.history)\n","  hist['epoch'] = history.epoch\n","\n","  plt.figure()\n","  plt.xlabel('Epoch')\n","  plt.ylabel('Mean Abs Error [MPG]')\n","  plt.plot(hist['epoch'], hist['mae'],\n","           label='Train Error')\n","  plt.plot(hist['epoch'], hist['val_mae'],\n","           label = 'Val Error')\n","  plt.ylim([0,5])\n","  plt.legend()\n","\n","  plt.figure()\n","  plt.xlabel('Epoch')\n","  plt.ylabel('Mean Square Error [$MPG^2$]')\n","  plt.plot(hist['epoch'], hist['mse'],\n","           label='Train Error')\n","  plt.plot(hist['epoch'], hist['val_mse'],\n","           label = 'Val Error')\n","  plt.ylim([0,20])\n","  plt.legend()\n","  plt.show()\n","\n","\n","plot_history(history)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"AqsuANc11FYv"},"source":["You can see that even though we defined 1000 epochs, the model only trained for around 70. That's because our EarlyStopping stopped the training once validation stopped improving. \n","\n","You can learn more about this callback [here](https://www.tensorflow.org/versions/master/api_docs/python/tf/keras/callbacks/EarlyStopping)."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"3St8-DmrX8P4"},"source":["The graph shows that on the validation set, the average error is usually around +/- 2 MPG. This means that our model predictions  were about + or - 2 MPG off.\n","\n","Let's see how well the model generalizes by using the **test** set, which we did not use when training the model.  This tells us how well we can expect the model to predict when we use it in the real world."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"jl_yNr5n1kms","colab":{}},"source":["loss, mae, mse = model.evaluate(normed_test_data, test_labels, verbose=0)\n","\n","print(\"Testing set Mean Abs Error: {:5.2f} MPG\".format(mae))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ft603OzXuEZC"},"source":["### Make predictions\n","\n","Finally, predict MPG values using data in the testing set. The following code will graph our model's best fit line compared to the acutal MPG values. You can see our model's best fit line 'fits' well to the testing data. "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Xe7RXH3N3CWU","colab":{}},"source":["test_predictions = model.predict(normed_test_data).flatten()\n","\n","plt.scatter(test_labels, test_predictions)\n","plt.xlabel('True Values [MPG]')\n","plt.ylabel('Predictions [MPG]')\n","plt.axis('equal')\n","plt.axis('square')\n","plt.xlim([0,plt.xlim()[1]])\n","plt.ylim([0,plt.ylim()[1]])\n","_ = plt.plot([-100, 100], [-100, 100])\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"19wyogbOSU5t"},"source":["It looks like our model predicts reasonably well. Let's take a look at the error distribution."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"f-OHX4DiXd8x","colab":{}},"source":["error = test_predictions - test_labels\n","plt.hist(error, bins = 25)\n","plt.xlabel(\"Prediction Error [MPG]\")\n","_ = plt.ylabel(\"Count\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"m0CB5tBjSU5w"},"source":["This last graph shows us the distribution  of prediction error. We can see that our model is pretty close to the money, only one answer throwing a skewed result. "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"vgGQuV-yqYZH"},"source":["## Conclusion\n","\n","This notebook introduced a few techniques to handle a regression problem.\n","\n","* Mean Squared Error (MSE) is a common loss function used for regression problems (different loss functions are used for classification problems).\n","* Similarly, evaluation metrics used for regression differ from classification. A common regression metric is Mean Absolute Error (MAE).\n","* When numeric input data features have values with different ranges, each feature should be scaled independently to the same range.\n","* If there is not much training data, one technique is to prefer a small network with few hidden layers to avoid overfitting.\n","* Early stopping is a useful technique to prevent overfitting."]}]}